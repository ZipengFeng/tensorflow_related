{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上个Demo中,使用了Softmax Regression模型进行mnist数据集的分类. 这是最简单的模型,但它与神经网络的最大区别是没有隐含层,因此拟合能力不足. 本例程开始搭建具有隐含层的神经网络.\n",
    "### 神经网络在使用中常常会出现下面几个问题, 因此需要添加一些参数进行调节. 本例程会把这些参数都添加进去,尽可能提高神经网络的分类准确率.\n",
    "#### 1.过拟合. 表示模型在训练时分类准确度高,在测试集上准确度较低, 推广能力差的情况. 可以引入dropout参数, 随机去除训练样本中的某些特征,可理解为对特征的一种采样.\n",
    "#### 2.参数调试困难. 神经网络不是凸优化问题,它处处充满了局部最优, 不同的学习速率常常会导致落入截然不同的局部最优中. 我们希望起初学习速率大一些, 加速收敛;后期学习速率小一些,可以稳定落入一个局部最优解. 为此设计出了Adagrad, Adam等自适应方法, 经实验都比SGD效果要好.\n",
    "#### 3.梯度弥散问题. 之前神经网络最常用sigmoid激活函数, 但在多层的传递后梯度会急剧减小,训练效果很差. 为此引入了Relu函数, 当某个神经元的信号在超过某个阈值时才被激活,平常处于被抑制状态.这样即使隐含层数很多,也不会出现梯度弥散问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_DATA\",one_hot = True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代0次,训练准确率为:0.53\n",
      "迭代500次,训练准确率为:0.89\n",
      "迭代1000次,训练准确率为:0.93\n",
      "迭代1500次,训练准确率为:0.98\n",
      "迭代2000次,训练准确率为:0.97\n",
      "迭代2500次,训练准确率为:0.99\n",
      "迭代3000次,训练准确率为:0.95\n",
      "迭代3500次,训练准确率为:0.97\n",
      "迭代4000次,训练准确率为:0.98\n",
      "迭代4500次,训练准确率为:0.93\n",
      "迭代5000次,训练准确率为:0.98\n",
      "迭代5500次,训练准确率为:0.99\n",
      "迭代6000次,训练准确率为:1.0\n",
      "迭代6500次,训练准确率为:1.0\n",
      "迭代7000次,训练准确率为:0.98\n",
      "迭代7500次,训练准确率为:1.0\n",
      "迭代8000次,训练准确率为:0.98\n",
      "迭代8500次,训练准确率为:1.0\n",
      "迭代9000次,训练准确率为:1.0\n",
      "迭代9500次,训练准确率为:1.0\n",
      "整体测试准确率为:0.9784\n"
     ]
    }
   ],
   "source": [
    "# 创建会话.后续过程都在该会话中进行\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "input_size = 784  # 输入神经元的个数,等于输入数据的特征维数\n",
    "hidden_size = 300  # 隐含层神经元的个数,需要调参取最优值\n",
    "\n",
    "# 隐含层的参数\n",
    "# 因为隐含层使用激活函数Relu,需要避免零梯度现象,不能将初始值赋为0.此处使用正态分布,标准差设为0.1\n",
    "W1 = tf.Variable(tf.truncated_normal([input_size,hidden_size],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "\n",
    "# 输出层的参数.输出层仍采用Softmax,初始值仍全赋为0即可.\n",
    "W2 = tf.Variable(tf.zeros([hidden_size,10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "x = tf.placeholder(tf.float32,[None,input_size])\n",
    "\n",
    "# 优化1:加入dropout(数值表示需要保留节点的概率).要注意的是,dropout在训练和测试时是不同的.\n",
    "# 训练时为防止过拟合,应当小于1;测试时需要用到全部特征,应当等于1.因此也需要一个占位符,在训练和测试时可灵活调节\n",
    "drop_out = tf.placeholder(tf.float32)\n",
    "\n",
    "# 定义隐含层的模型结构.\n",
    "# 优化2:加入relu.公式为:hidden = relu(w1*x+b1).再加上dropout.\n",
    "hidden = tf.nn.relu(tf.matmul(x,W1) + b1)\n",
    "hidden_drop = tf.nn.dropout(hidden,drop_out)\n",
    "\n",
    "# 输出层的模型结构与上个例程的softmax相同\n",
    "y = tf.nn.softmax(tf.matmul(hidden_drop,W2) + b2)\n",
    "\n",
    "# 定义loss函数.优化的目标是使该函数值尽量减小.使用常用的交叉熵作为损失函数.设置y_即真正的label值,用来计算交叉熵\n",
    "y_ = tf.placeholder(tf.float32,[None,10])\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "\n",
    "# 优化3:将SGD寻优算法改为Adam.learning_rate要通过不断调试才能找到最合适的值.\n",
    "#       最初设置的为0.5,效果特别差.\n",
    "#       笔者也测试了Adagrad算法,learning_rate取0.3时效果几乎一致.\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# 全局参数初始化\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# 设置准确率函数.当预测值与实际值相同时,即为分类准确.然后求整体的准确率\n",
    "correct_pred = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "# 喂参数,开始训练.每次随机抽取100条样本进行训练,共迭代1000次.注意:输入的y值是真实标签,应当是y_! y是输出值\n",
    "for i in range(10000):\n",
    "    batch_x, batch_y = mnist.train.next_batch(100)\n",
    "    optimizer.run({x:batch_x,y_:batch_y,drop_out:0.75})   #训练时drop_out设置为0.75\n",
    "\n",
    "    if i%500 == 0:\n",
    "        print \"迭代\"+ str(i) + \"次,训练准确率为:\" + str(acc.eval({x:batch_x,y_:batch_y,drop_out:0.75}))\n",
    "        \n",
    "#     如上的run()操作也可写为:sess.run(optimizer,feed_dict={x:batch_x,y_:batch_y})\n",
    "#                        与:sess.run(acc,feed_dict={x:batch_x,y_:batch_y})\n",
    "#     这样看起来更清晰.在执行算法定义的代码时,计算并没有发生.只有调用run方法并feed数据时才真正执行.\n",
    "\n",
    "\n",
    "print \"整体测试准确率为:\" + str(acc.eval({x:mnist.test.images,y_:mnist.test.labels,drop_out:1.0}))   #测试时drop_out设置为1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
