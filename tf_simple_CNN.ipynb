{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 本例程开始搭建深度神经网络之一的卷积神经网络.\n",
    "### 卷积神经网络由多个卷积层构成. 每个卷积层进行如下操作:\n",
    "(1)图像通过多个卷积核进行卷积操作,提取出局部特征,每个卷积核会映射出一个新的2D图像.并对输出结果进行Relu等激活函数处理.\n",
    "(2)池化,减少参数.最常采用最大池化以保留最显著的特征.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_DATA\",one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代0次,训练准确率为:0.12\n",
      "迭代500次,训练准确率为:0.86\n",
      "迭代1000次,训练准确率为:0.94\n",
      "迭代1500次,训练准确率为:0.96\n",
      "迭代2000次,训练准确率为:0.98\n",
      "迭代2500次,训练准确率为:0.96\n",
      "迭代3000次,训练准确率为:0.98\n",
      "迭代3500次,训练准确率为:0.96\n",
      "迭代4000次,训练准确率为:1.0\n",
      "迭代4500次,训练准确率为:1.0\n",
      "迭代5000次,训练准确率为:1.0\n",
      "迭代5500次,训练准确率为:0.98\n",
      "迭代6000次,训练准确率为:1.0\n",
      "迭代6500次,训练准确率为:0.98\n",
      "迭代7000次,训练准确率为:1.0\n",
      "迭代7500次,训练准确率为:0.98\n",
      "迭代8000次,训练准确率为:1.0\n",
      "迭代8500次,训练准确率为:0.96\n",
      "迭代9000次,训练准确率为:1.0\n",
      "迭代9500次,训练准确率为:1.0\n",
      "迭代10000次,训练准确率为:0.98\n",
      "迭代10500次,训练准确率为:0.98\n",
      "迭代11000次,训练准确率为:1.0\n",
      "迭代11500次,训练准确率为:1.0\n",
      "迭代12000次,训练准确率为:1.0\n",
      "迭代12500次,训练准确率为:1.0\n",
      "迭代13000次,训练准确率为:1.0\n",
      "迭代13500次,训练准确率为:0.96\n",
      "迭代14000次,训练准确率为:1.0\n",
      "迭代14500次,训练准确率为:1.0\n",
      "迭代15000次,训练准确率为:1.0\n",
      "迭代15500次,训练准确率为:1.0\n",
      "迭代16000次,训练准确率为:0.98\n",
      "迭代16500次,训练准确率为:0.98\n",
      "迭代17000次,训练准确率为:1.0\n",
      "迭代17500次,训练准确率为:1.0\n",
      "迭代18000次,训练准确率为:1.0\n",
      "迭代18500次,训练准确率为:1.0\n",
      "迭代19000次,训练准确率为:1.0\n",
      "迭代19500次,训练准确率为:1.0\n",
      "整体测试准确率为:0.986\n"
     ]
    }
   ],
   "source": [
    "# 创建会话.后续过程都在该会话中进行\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 深度神经网络权值较多,首先定义一个初始化函数.为克服梯度为零和死亡节点的问题,将初始权重设为正态分布,偏置设为0.1\n",
    "def weight_variable (shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable (shape):\n",
    "    initial = tf.constant(0.1,shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 同样,卷积和池化操作也是多次用到,也定义一下\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides = [1,1,1,1],padding = 'SAME')\n",
    "# 数字代表卷积模板移动的步长.padding为边界处理方式,'same'代表输入和输出保持同样尺寸\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize = [1,2,2,1],strides = [1,2,2,1],padding = 'SAME')\n",
    "# 池化并缩小尺寸,横竖步长都为2\n",
    "\n",
    "# 充分利用图像结构信息,将输入尺寸变为28*28*1,1代表通道数量\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y_ = tf.placeholder(tf.float32,[None,10])\n",
    "x_reshape = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "# 第一个卷积层\n",
    "w_conv1 = weight_variable([5,5,1,32])  #卷积核尺寸为5*5,1个颜色通道,32个卷积核即32个特征\n",
    "b_conv1 = bias_variable([32])\n",
    "y_conv1 = tf.nn.relu(conv2d(x_reshape,w_conv1) + b_conv1) #卷积和relu操作\n",
    "y_pool1 = max_pool_2x2(y_conv1)   #池化操作\n",
    "\n",
    "\n",
    "# 第二个卷积层\n",
    "w_conv2 = weight_variable([5,5,32,64])  #卷积核尺寸为5*5,64个卷积核即64个特征\n",
    "b_conv2 = bias_variable([64])\n",
    "y_conv2 = tf.nn.relu(conv2d(y_pool1,w_conv2) + b_conv2) #卷积和relu操作\n",
    "y_pool2 = max_pool_2x2(y_conv2)   #池化操作\n",
    "\n",
    "# 全连接层\n",
    "# 经过两次2*2的池化操作,特征数量为64,则输出尺寸为7*7*64.卷积核定为1024个\n",
    "# 在全连接层前需要将数据转化为1维的.\n",
    "w_fc1 = weight_variable([7*7*64,1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "y_pool2 = tf.reshape(y_pool2,[-1,7*7*64])\n",
    "y_fc1 = tf.nn.relu(tf.matmul(y_pool2,w_fc1) + b_fc1)\n",
    "\n",
    "# 添加dropout.一般来说,dropout常加在全连接层\n",
    "drop_out = tf.placeholder(tf.float32)\n",
    "y_fc1_drop = tf.nn.dropout(y_fc1,drop_out)\n",
    "\n",
    "# 输出层.仍使用softmax函数\n",
    "w_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y = tf.nn.softmax(tf.matmul(y_fc1_drop,w_fc2) + b_fc2)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "\n",
    "# 将SGD寻优算法改为Adam.learning_rate要通过不断调试才能找到最合适的值.\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# 全局参数初始化\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# 设置准确率函数.当预测值与实际值相同时,即为分类准确.然后求整体的准确率\n",
    "correct_pred = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "# 喂参数,开始训练.每次随机抽取100条样本进行训练,共迭代1000次.注意:输入的y值是真实标签,应当是y_! y是输出值\n",
    "for i in range(20000):\n",
    "    batch_x, batch_y = mnist.train.next_batch(50)\n",
    "    optimizer.run({x:batch_x,y_:batch_y,drop_out:0.5})   #训练时drop_out设置为0.5\n",
    "\n",
    "    if i%500 == 0:\n",
    "        print \"迭代\"+ str(i) + \"次,训练准确率为:\" + str(acc.eval({x:batch_x,y_:batch_y,drop_out:1.0}))\n",
    "        \n",
    "#     如上的run()操作也可写为:sess.run(optimizer,feed_dict={x:batch_x,y_:batch_y})\n",
    "#                        与:sess.run(acc,feed_dict={x:batch_x,y_:batch_y})\n",
    "#     这样看起来更清晰.在执行算法定义的代码时,计算并没有发生.只有调用run方法并feed数据时才真正执行.\n",
    "\n",
    "# 由于内存限制,只选取了测试集中的2000张进行测试\n",
    "batch_xt, batch_yt = mnist.test.next_batch(2000)\n",
    "print \"整体测试准确率为:\" + str(acc.eval(feed_dict = {x:batch_xt,y_:batch_yt,drop_out:1.0}))   #测试时drop_out设置为1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
